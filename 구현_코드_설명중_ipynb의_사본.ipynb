{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "구현 코드 설명중.ipynb의 사본",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/syeong1218/python/blob/master/%EA%B5%AC%ED%98%84_%EC%BD%94%EB%93%9C_%EC%84%A4%EB%AA%85%EC%A4%91_ipynb%EC%9D%98_%EC%82%AC%EB%B3%B8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dPLswJL0il3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import 할 것들\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import Draw, Descriptors\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMAeHMkg0zZc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "smifile = \"GDBChEMBL\".smi\"   # GDBChEMBL 의 SMILES 데이터 세트를 사용합니다.\n",
        "data = pd.read_csv(smifile, delimiter = \"\\t\", names = [\"smiles\",\"No\",\"Int\"])\n",
        "from sklearn.cross_validation import train_test_split  #사이킷런을 import 합니다.\n",
        "smiles_train, smiles_test = train_test_split(data[\"smiles\"], random_state=42) \n",
        "print(smiles_train.shape) # 훈련데이터와 시험데이터를 출력합니다.\n",
        "print(smiles_test.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJmhi12P2TKT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "charset = set(\"\".join(list(data.smiles))+\"!E\") \n",
        "char_to_int = dict((c,i) for i,c in enumerate(charset))\n",
        "int_to_char = dict((i,c) for i,c in enumerate(charset))\n",
        "embed = max([len(smile) for smile in data.smiles]) + 5\n",
        "print str(charset)\n",
        "print(len(charset), # RNN이 배치 모드에서 학습되고 최대 발생 + 일부 추가된 발생으로 설정되므로 SMILES 문자열의 최대 길이가 필요"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HogAT_j-6nh1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def vectorize(smiles):  # 함수를 정의\n",
        "        one_hot =  np.zeros((smiles.shape[0], embed , len(charset)),dtype=np.int8) #\n",
        "        for i,smile in enumerate(smiles):\n",
        "            #encode the startchar\n",
        "            one_hot[i,0,char_to_int[\"!\"]] = 1  # 원 핫 인코딩을 수행\n",
        "            #encode the rest of the chars\n",
        "            for j,c in enumerate(smile):\n",
        "                one_hot[i,j+1,char_to_int[c]] = 1  # 원 핫 인코딩을 수행\n",
        "            #Encode endchar\n",
        "            one_hot[i,len(smile)+1:,char_to_int[\"E\"]] = 1  # 원 핫 인코딩을 수행\n",
        "        #Return two, one for input and the other for output\n",
        "        return one_hot[:,0:-1,:], one_hot[:,1:,:]\n",
        "X_train, Y_train = vectorize(smiles_train.values) # X , Y 훈련 데이터를 벡터화\n",
        "X_test,Y_test = vectorize(smiles_test.values) # X , Y 시험 데이터를 벡터화\n",
        "print (smiles_train.iloc[0])  # 값을 출력\n",
        "plt.matshow(X_train[0].T)\n",
        "#print X_train.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKctL3_M8cxX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Keras 를 import함\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Concatenate\n",
        "from keras import regularizers\n",
        "input_shape = X_train.shape[1:]\n",
        "output_dim = Y_train.shape[-1]\n",
        "latent_dim = 64\n",
        "lstm_dim = 64"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgL3Zeq-8xmN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "unroll = False # RNN의 속도를 높이기 위해 False로 지정\n",
        "encoder_inputs = Input(shape=input_shape)  # 인코더에 입력\n",
        "encoder = LSTM(lstm_dim, return_state=True,\n",
        "                unroll=unroll) # 위에서 lstm_dim이 64라고했으므로 64LSTM 셀의 단일층은 입력SMILES 문자열을 읽는 데 사용\n",
        "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
        "states = Concatenate(axis=-1)([state_h, state_c])\n",
        "neck = Dense(latent_dim, activation=\"relu\")\n",
        "neck_outputs = neck(states)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_Av5vewB-83",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decode_h = Dense(lstm_dim, activation=\"relu\")\n",
        "decode_c = Dense(lstm_dim, activation=\"relu\")\n",
        "state_h_decoded =  decode_h(neck_outputs)\n",
        "state_c_decoded =  decode_c(neck_outputs)\n",
        "encoder_states = [state_h_decoded, state_c_decoded]\n",
        "decoder_inputs = Input(shape=input_shape)\n",
        "decoder_lstm = LSTM(lstm_dim,\n",
        "                    return_sequences=True,  #각 sequence마다 출력값을 출력. LSTM 레이어를 여러개로 쌓아올릴 때는 return_sequence=True 옵션을 사용\n",
        "                    unroll=unroll\n",
        "                   )\n",
        "decoder_outputs = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
        "decoder_dense = Dense(output_dim, activation='softmax')  # Decoder 층에 마지막에 softmax 활성화함수 사용\n",
        "decoder_outputs = decoder_dense(decoder_outputs)  # 출력\n",
        "#두 위치에 대해 훈련 벡터를 입력하고 입력에 앞서 한 문자를 예측하는 모델 정의\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "print model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eir63KJlICXd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import History, ReduceLROnPlateau # History : 케라스는 학습시킬 때 fit함수를 사용하고 History 객체가 반환됩니다. History는 이어지는 epoch 각각에서의 훈련 손실 값과 metric 값을 기록합니다.\n",
        "h = History()\n",
        "rlr = ReduceLROnPlateau(monitor='val_loss', factor=0.5,patience=10, min_lr=0.000001, verbose=1, epsilon=1e-5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sp_zxoFaJxwM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "pickle.dump(h.history, file(\"Blog_history.pickle\",\"w\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qmmx7I8KCsp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(h.history[\"loss\"], label=\"Loss\")\n",
        "plt.plot(h.history[\"val_loss\"], label=\"Val_Loss\")\n",
        "plt.yscale(\"log\")\n",
        "plt.legend()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tut-ZfmmKLU4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 아래 코드는 아무것도 출력하지 않으므로 테스트 세트에서 100 개의 테스트 된 SMILES가 완벽하게 재구성됩니다.\n",
        "for i in range(100):\n",
        "    v = model.predict([X_test[i:i+1], X_test[i:i+1]]) #Can't be done as output not necessarely 1 , 모델을 시험데이터로 예측\n",
        "    idxs = np.argmax(v, axis=2)\n",
        "    pred=  \"\".join([int_to_char[h] for h in idxs[0]])[:-1]\n",
        "    idxs2 = np.argmax(X_test[i:i+1], axis=2)\n",
        "    true =  \"\".join([int_to_char[k] for k in idxs2[0]])[1:]\n",
        "    if true != pred:\n",
        "        print true, pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFj5nb7JNgVf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "smiles_to_latent_model = Model(encoder_inputs, neck_outputs) # 숨어있는 모델 (은닉모델)\n",
        "\n",
        "smiles_to_latent_model.save(\"Blog_simple_smi2lat.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYoQSrLiQOcE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "latent_input = Input(shape=(latent_dim,)) #은닉층 입력\n",
        "#reuse_layers : 잠재 공간과 일치하는 새로운 입력이 정의되었지만 이전의 층을 재사용하여 h(hidden) 및 c(cell) 상태를 얻을 수 있습니다. 그렇게하면 가중치가 훈련 된 모델에서 상속됩니다.\n",
        "state_h_decoded_2 =  decode_h(latent_input)\n",
        "state_c_decoded_2 =  decode_c(latent_input)\n",
        "latent_to_states_model = Model(latent_input, [state_h_decoded_2, state_c_decoded_2])\n",
        "latent_to_states_model.save(\"Blog_simple_lat2state.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3qmtPSWSkVq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Last one is special, we need to change it to stateful, and change the input shape\n",
        "inf_decoder_inputs = Input(batch_shape=(1, 1, input_shape[1]))\n",
        "inf_decoder_lstm = LSTM(lstm_dim,\n",
        "                    return_sequences=True,\n",
        "                    unroll=unroll,\n",
        "                    stateful=True # 상태유지 LSTM 모델. 여기서 상태유지라는 것은 현재 학습된 상태가 다음 학습시 초기 상태로 전달된다는 것을 의미\n",
        "                   )                # 마지막 샘플 학습이 마치고, 새로운 에포크 수행 시에는 새로운 샘플 학습을 해야하므로 상태 초기화 필요\n",
        "inf_decoder_outputs = inf_decoder_lstm(inf_decoder_inputs)\n",
        "inf_decoder_dense = Dense(output_dim, activation='softmax') # 마지막 출력할 때는 소프트맥스 함수로 확률로 변환시킴\n",
        "inf_decoder_outputs = inf_decoder_dense(inf_decoder_outputs)\n",
        "sample_model = Model(inf_decoder_inputs, inf_decoder_outputs)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u64qAs3ZTzC9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Transfer Weights : 디코더 모델을 정의한 후 학습 된 자동 인코더 모델에서 해당 가중치가 전송됩니다.\n",
        "for i in range(1,3):\n",
        "    sample_model.layers[i].set_weights(model.layers[i+6].get_weights())\n",
        "sample_model.save(\"Blog_simple_samplemodel.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5NGqGnXVKjt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_latent = smiles_to_latent_model.predict(X_test) # 잠재 모델에 대한 스마일은 잠재 공간과 같은 SMILES를 fingerprint로 인코딩하는데 사용할 수 있습니다."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkojK9r9XxzL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 비슷한 분자들은 비슷한 fingerprint를 만든다. 숨겨진 공간에서 비슷한 분자들이 비슷한 벡터들을 만드는 것을 보기 위해서 비슷한 분자들에 대한 간단한 검색의 수행된다. \n",
        "# 숨겨진 벡터간의 절대값의 차이는 행렬의 유사성을 이용한다. 이 테스트는 빠르게 수행될 수 있다.\n",
        "# 여기서 분자 구조가 비슷한 분자들은 물성이 비슷하다는 과학적인 지식에서 출발한 개념이다.\n",
        "molno = 5\n",
        "latent_mol = smiles_to_latent_model.predict(X_test[molno:molno+1])\n",
        "sorti = np.argsort(np.sum(np.abs(x_latent - latent_mol), axis=1))\n",
        "print sorti[0:10]\n",
        "print smiles_test.iloc[sorti[0:8]]\n",
        "Draw.MolsToImage(smiles_test.iloc[sorti[0:8]].apply(Chem.MolFromSmiles))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7jntOtEc90C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 위에서 구현한 내용을 그림으로 나타내 보면\n",
        "Draw.MolsToImage(smiles_test.iloc[sorti[-8:]].apply(Chem.MolFromSmiles))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "759Wk2e3dVIu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 이온화되지 않은 화합물의 농도 비율을 logP라 한다. 분배계수(Partition-coefficient)는 섞이지 않는 2종류의 용매 혼합물에 화합물을 섞고 평형상태가 이루어졌을 때 각 용매에서의 화합물 농도 비율을 의미합니다.\n",
        "# Octanol과 물에 대한 화합물의 분배계수를 통해 지질친화도를 평가할 수 있기 때문에 신약개발에서 중요하게 보는 물리화학적 성질입니다. \n",
        "# 농도 비율에 log를 취한 값을 보통 사용하는데, 이온화 되지 않은 화합물의 농도 비율을 logP라 합니다.\n",
        "logp = smiles_test.apply(Chem.MolFromSmiles).apply(Descriptors.MolLogP)\n",
        "# 사이킷런 import\n",
        "from sklearn.decomposition import PCA  # PCA 를 이용해서 쉽게 차원을 축소시키고, 새로 만들어진 데이터로 dataframe 을 만들어줍니다.\n",
        "pca = PCA(n_components = 2)\n",
        "red = pca.fit_transform(x_latent)\n",
        "plt.figure()\n",
        "plt.scatter(red[:,0], red[:,1],marker='.', c= logp)\n",
        "print(pca.explained_variance_ratio_, np.sum(pca.explained_variance_ratio_))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PjGw4ht7iiji",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "molwt = smiles_test.apply(Chem.MolFromSmiles).apply(Descriptors.MolMR)\n",
        "plt.figure()\n",
        "plt.scatter(red[:,0], red[:,1],marker='.', c= molwt)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adOsMCxeinJ9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Model LogP?\n",
        "x_train_latent = smiles_to_latent_model.predict(X_train)\n",
        "logp_train = smiles_train.apply(Chem.MolFromSmiles).apply(Descriptors.MolLogP)\n",
        "\n",
        "from keras.models import Sequential\n",
        "logp_model = Sequential()\n",
        "logp_model.add(Dense(128, input_shape=(latent_dim,), activation=\"relu\"))\n",
        "logp_model.add(Dense(128, activation=\"relu\"))\n",
        "logp_model.add(Dense(1))\n",
        "logp_model.compile(optimizer=\"adam\", loss=\"mse\")\n",
        "\n",
        "rlr = ReduceLROnPlateau(monitor='val_loss', factor=0.5,patience=10, min_lr=0.000001, verbose=1, min_delta=1e-5)\n",
        "logp_model.fit(x_train_latent, logp_train, batch_size=128, epochs=400, callbacks = [rlr]) #callbacks : 정한 epoch 또는 1 epoch이 끝날때마다 이 함수를 불러서 정해놓은 기능"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "09Vs5ZbYucah",
        "colab": {}
      },
      "source": [
        "logp_pred_train = logp_model.predict(x_train_latent)\n",
        "logp_pred_test = logp_model.predict(x_latent)\n",
        "plt.scatter(logp, logp_pred_test, label=\"Test\")\n",
        "plt.scatter(logp_train, logp_pred_train, label=\"Train\")\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "34a96a9Ouca6",
        "colab": {}
      },
      "source": [
        "#은닉 공간에서 smiles\n",
        "def latent_to_smiles(latent):\n",
        "    #decode states and set Reset the LSTM cells with them\n",
        "    states = latent_to_states_model.predict(latent)\n",
        "    sample_model.layers[1].reset_states(states=[states[0],states[1]])\n",
        "    #Prepare the input char\n",
        "    startidx = char_to_int[\"!\"]\n",
        "    samplevec = np.zeros((1,1,22))\n",
        "    samplevec[0,0,startidx] = 1\n",
        "    smiles = \"\"\n",
        "    #Loop and predict next char\n",
        "    for i in range(28):\n",
        "        o = sample_model.predict(samplevec)\n",
        "        sampleidx = np.argmax(o)\n",
        "        samplechar = int_to_char[sampleidx]\n",
        "        if samplechar != \"E\":\n",
        "            smiles = smiles + int_to_char[sampleidx]\n",
        "            samplevec = np.zeros((1,1,22))\n",
        "            samplevec[0,0,sampleidx] = 1\n",
        "        else:\n",
        "            break\n",
        "    return smiles"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5ayNY_dcucbJ",
        "colab": {}
      },
      "source": [
        "smiles = latent_to_smiles(x_latent[0:1])\n",
        "print(smiles)\n",
        "print(smiles_test.iloc[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "o_d6S8w5ucbY",
        "colab": {}
      },
      "source": [
        "wrong = 0\n",
        "for i in range(1000):\n",
        "    smiles = latent_to_smiles(x_latent[i:i+1])\n",
        "    mol = Chem.MolFromSmiles(smiles)\n",
        "    if mol:\n",
        "        pass\n",
        "    else:\n",
        "        print(smiles)\n",
        "        wrong = wrong + 1\n",
        "print (\"%0.1F percent wrongly formatted smiles\"%(wrong/float(1000)*100))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-H-ApcYEucbp",
        "colab": {}
      },
      "source": [
        "#Interpolation test in latent_space\n",
        "i = 0\n",
        "j= 2\n",
        "latent1 = x_latent[j:j+1]\n",
        "latent0 = x_latent[i:i+1]\n",
        "mols1 = []\n",
        "ratios = np.linspace(0,1,25)\n",
        "for r in ratios:\n",
        "    #print r\n",
        "    rlatent = (1.0-r)*latent0 + r*latent1\n",
        "    smiles  = latent_to_smiles(rlatent)\n",
        "    mol = Chem.MolFromSmiles(smiles,sanitize=False)\n",
        "    if mol:\n",
        "        mols1.append(mol)\n",
        "    else:\n",
        "        print(smiles)\n",
        "Draw.MolsToGridImage(mols1, molsPerRow=5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Rmj3cYJlucb8",
        "colab": {}
      },
      "source": [
        "#Sample around the latent vector \n",
        "latent = x_latent[0:1]\n",
        "scale = 0.40\n",
        "mols = []\n",
        "for i in range(20):\n",
        "    latent_r = latent + scale*(np.random.randn(latent.shape[1])) #TODO, try with\n",
        "    smiles = latent_to_smiles(latent_r)\n",
        "    mol = Chem.MolFromSmiles(smiles)\n",
        "    if mol:\n",
        "        mols.append(mol)\n",
        "    else:\n",
        "        print(smiles)\n",
        "Draw.MolsToGridImage(mols, molsPerRow=5)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}